\chapter{Implementation}
\label{sec:impl}

There are a few things to note about how implementation would work in
practice. First, NTP requires a few thousand seconds to settle,
because it not only works with last-seen error but also estimated
drift rate, changes in that drift rate. Thus, a new machine that is
being added to the cluster could not be used in snapshots until
roughly a few thousand seconds after it was added to the
cluster. Probably this would be handled by giving new machines a ``warm
up" period before they are actually assigned data. Once the machine's
NTP estimates settle, it can be incorporated into the Ceph cluster.

The quality of the master NTP clock significantly affects the size
required for safe freeze windows, as we saw in the simulation
results. We recommend that a GPS, atomic, or other highly precise
clock (any device capable as acting as an NTP stratum 0) be used
instead of the on-board RTC for that machine. This allows the master
clock to function as a stratum 1 clock, eliminating NTP uncertainty
due to upstream variation. This is not strictly necessary, and
performance is generally acceptable with a more distant stratum, but a
stratum 1 clock will provide the best performance for the
cluster. Without a stratum 0 device directly connect to the NTP
stratum 1 server, NTP is forced to assume to worst possible clock
properties for the on-board RTC.

Another time protocol could be used in place of NTP, and ideally one
more tailored to data center precision would in fact be usable. NTP
was used for analysis here because it is widely deployed, it can bound
its own uncertainty, and the reference implementation of the protocol
provides easy access to important diagnostic information. It is most
likely possible to implement the required uncertainty calculation in
PTP, but this would require extensions beyond the specification of the
protocol. Chrony, another NTP implementation, could likely also be
modified to report the necessary information.

There are a few fault conditions that can occur during a snapshot. The
simplest, the primary NTP master clock going down, would require a
short suspension of snapshotting (a few hundred to a few thousand
seconds, depending on the specific characteristics of the clocks)
until all clocks in the data center re-settled on the new master
clock. Scenarios with more than one master clock on a network have not
been analyzed.

\section{Snapshot Validation}

As in all systems, hardware failures are possible. As a result, we
must consider how to mitigate against a network disruption or clock
hardware failure leading to a significant clock desynchronization of
an individual or group of nodes. This would be detectable via a major
spike in various diagnostic outputs from NTP (different kinds of
uncertainty and measures of clock drift and wander. %% TODO explain this
The node would report this event either within a heartbeat message or
through an extra, priority message to one of the monitors. The
monitors would then be able to check that all of the failed node's PGs
had replicas that did not experience such a failure. As shown in the
proof of correctness section, all correctly-behaving nodes have an
overlapping good time, so if at least one PG replica behaved
correctly, at least one would have prevented any writes from becoming
visible (the issue covered in the problem discussion TODO
REFERENCE). If there is at least one good node in each PG replica set,
the snapshot is still safe.

