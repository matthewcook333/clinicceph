\chapter{Approach}
\label{sec:approach}

We have developed a simple, performant clock-synchronization-based
algorithm that can provide consistent data center level snapshots.

The algorithm has a few requirements. First, it requires that the
clock synchronization algorithm used by the data center support error
bounding. The most common NTP implementation, \texttt{ntpd}, supports
this. Second, to remain performant it requires a certain baseline
clock and network link quality, although it will degrade gracefully
with poor clocks or links (Chapter~\ref{sec:results})

We propose an algorithm composed of 4 major phases:

\begin{enumerate}

\item \emph{Synchronization}

  During the synchronization phase, all nodes run a supported time synchronization
  algorithm. All nodes attempts to synchronize their internal clocks 
  with a single, common master clock. Administrative messages are also 
  exchanged during this phase. These messages include scheduling future 
  snapshot times. These messages may also be able to be rolled into already
  existing messages such as map updates or heartbeats. 

\item \emph{Freeze}
  
  When a node is frozen, it holds write confirmations. Incoming writes may 
  be processed, but
  completion is not acknowledged until the end of the freeze. 
  
  Let $U_i$ be the uncertainty in node
  $i$'s current time, and $T$ be the scheduled snapshot time. Node $i$
  begins its freeze when its clock reads $T - U_i$, and completes it
  when its clock reads $T + U_i$, guaranteeing that the master clock's
  $T$ is captured in the freeze window for all $i$ (Chapter~\ref{sec:proof}).

\item \emph{Confirmation}

  Before a snapshot may be marked as successful, the data center must wait a
  short period. This allows any sudden clock desynchronization events
  or node failures to be detected and consistency
  checked. The Ceph cluster can verify that, for every node failure during
  the snapshot, at
  least one other replica of that node's data did not fail. This is 
  enough data to recover a snapshot (Chapter~\ref{sec:proof}).
  If no irreparable inconsistencies are found, the
  snapshot is marked as successful.

\item \emph{Replication}
  
  Finally, if the snapshot was marked as successful after confirmation, 
  the data in the Ceph cluster, as it existed at the time of the snapshot, 
  may begin being replicated to a remote Ceph cluster. 
  Because the first step in taking a snapshot is
  simply writing down a marker in an OSD's log, it is straightforward
  either to do a full snapshot, or just a diff since the last good
  snapshot.
  
  While replication is underway, the next synchronization phase begins.

\end{enumerate}
