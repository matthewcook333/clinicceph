\chapter{Introduction}
\label{sec:introduction}

Ceph is a highly distributed, strongly consistent file system. It
stores data redundantly within a data center using a distributed hashing
algorithm. This hashing algorithm determines how data is divided among
storage nodes.  By calculating hash information independently, a Ceph
client may communicate directly with the correct storage nodes rather
than relying on a separate controller node. The result of this
approach is a distributed control structure, free of single points of
failure. This allows Ceph clusters to be very robust. This
communication structure also has the benefit of improving overall
performance and scalability by spreading control and communication
load across all nodes.

For this project, we were tasked with determining a means by which to
asynchronously geo-replicate the contents of a Ceph cluster. The task
also specifies requirements for performance and consistency. This
required overcoming the following complications:

\begin{itemize}
\item Inability to perfectly synchronize clocks between machines
\item Lack of knowledge of causality relationships between reads and
  writes in the cluster, because Ceph permits out-of-band
  communication.
\item Insufficient per-node knowledge to fully reconstruct the state
  of a cluster at any specific time as Ceph is currently implemented.
\end{itemize}

Chapter~\ref{sec:proof} covers a theoretical proof and performance
analysis of our proposed algorithm by showing that short per-node holds on
incoming writes can, if NTP behaves within spec, provide guaranteed
safe snapshot points in time. Chapters~\ref{sec:analysis} and
\ref{sec:results} cover a concrete demonstration of the same,
finding that write holds of 3-20 milliseconds (depending on clock and
network quality) with a real NTP implementation do in fact provide
safe snapshot points. 

In addition to this report, we are also delivering a set of utilities
and scripts to replicate and extend our analysis. These scripts will
also be useful in analyzing clusters on which consistent
geo-replication is enabled. Using the results of this clinic project,
a future team should be able to quickly implement our algorithm in
Ceph.

In Appendix~\ref{sec:terms} on page~\pageref{sec:terms} we provide
a glossary including technical terms we use in this report.


%TODO makeing clear that we aren't delivering code
